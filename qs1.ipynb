## **Deep RL + Continuous State**  
### **Task**  
### Implement a **DQN (Deep Q-Network)** agent that processes **continuous inputs**:  
### - **`gaze_ratio`**: Float (0.0 to 1.0).  
### - **`tone_score`**: Float (-1.0 to +1.0, from sentiment analysis).  

### **Action Space**  
### - **`discount_percent`**: 0%, 5%, or 10%.  

### **Reward Function**  
### - **`+10`** if purchase, **`-2 * discount_percent`** if no purchase.  

### **Deliverables - source code**  
### 1. **Preprocess states** (normalize gaze/tone).  
### 2. **Implement DQN** with a 2-layer neural network.  
### 3. **Train for 50 episodes** and plot rewards over time.  

class DQNAgent:  
    def __init__(self, state_dim, action_dim):  
        self.model = self._build_model(state_dim, action_dim)  
        self.memory = []  # Experience replay buffer  

    def _build_model(self, state_dim, action_dim):  
        model = tf.keras.Sequential([  
            tf.keras.layers.Dense(16, activation='relu', input_shape=(state_dim,)),  
            tf.keras.layers.Dense(action_dim)  
        ])  
        model.compile(optimizer='adam', loss='mse')  
        return model  

    def act(self, state, epsilon=0.1):  
        if np.random.rand() < epsilon:  
            return np.random.choice([0, 5, 10])  # Random discount  
        q_values = self.model.predict(state[np.newaxis])  
        return np.argmax(q_values[0]) * 5  # 0%, 5%, or 10%  

    def train(self, batch_size=32):  
        # Sample from replay buffer and update DQN  
        pass 

    # Simulated training loop  
agent = DQNAgent(state_dim=2, action_dim=3)  
for episode in range(50):  
    state = np.array([np.random.random(), np.random.uniform(-1, 1)])  # [gaze, tone]  
    action = agent.act(state)  
    reward = simulate_response(state, action)  
    next_state = np.array([np.random.random(), np.random.uniform(-1, 1)])  
    agent.memory.append((state, action, reward, next_state))  
    agent.train()   